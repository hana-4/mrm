{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.init(project=\"Bert_pretraining\", name=\"init\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:47:22.738427Z","iopub.execute_input":"2025-03-27T11:47:22.738867Z","iopub.status.idle":"2025-03-27T11:47:27.984468Z","shell.execute_reply.started":"2025-03-27T11:47:22.738836Z","shell.execute_reply":"2025-03-27T11:47:27.982890Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9dd2370937d7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bert_pretraining\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"init\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the smaller WikiText-2 dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")  #wikitext is the family of datasets\n\n# Print dataset splits\nprint(dataset)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:57:15.677233Z","iopub.execute_input":"2025-03-27T11:57:15.677851Z","iopub.status.idle":"2025-03-27T11:57:19.872619Z","shell.execute_reply.started":"2025-03-27T11:57:15.677788Z","shell.execute_reply":"2025-03-27T11:57:19.871606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/685k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c1e0cd53b647ecbf54ebe04aa4ab41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de367f562f742ca9f31d1a3e6a22cb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/618k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"704a3225f38c4566b3694b92918ad845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6803b801004af9b677cf1d5c1eec14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e6acbb3aa042b0a74cc953c003055c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996b3978463b4723980cdfa881758b9c"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Check dataset size\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Validation size: {len(dataset['validation'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:02:31.139858Z","iopub.execute_input":"2025-03-27T12:02:31.141023Z","iopub.status.idle":"2025-03-27T12:02:31.150615Z","shell.execute_reply.started":"2025-03-27T12:02:31.140964Z","shell.execute_reply":"2025-03-27T12:02:31.149428Z"}},"outputs":[{"name":"stdout","text":"Train size: 36718\nValidation size: 3760\nTest size: 4358\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Compute average length of samples\navg_length = sum(len(sample[\"text\"].split()) for sample in dataset[\"train\"]) / len(dataset[\"train\"])\nprint(f\"Average words per sample: {avg_length:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:02:44.149816Z","iopub.execute_input":"2025-03-27T12:02:44.150188Z","iopub.status.idle":"2025-03-27T12:02:45.004351Z","shell.execute_reply.started":"2025-03-27T12:02:44.150161Z","shell.execute_reply":"2025-03-27T12:02:45.003192Z"}},"outputs":[{"name":"stdout","text":"Average words per sample: 55.88\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load a tokenizer (using GPT-2 as an example)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Tokenize a sample\nsample_text = dataset[\"train\"][1][\"text\"]\ntokens = tokenizer(sample_text, truncation=True, padding=\"max_length\", max_length=512)\n\n# Print tokenized output\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:04:38.372009Z","iopub.execute_input":"2025-03-27T12:04:38.372471Z","iopub.status.idle":"2025-03-27T12:04:38.712580Z","shell.execute_reply.started":"2025-03-27T12:04:38.372442Z","shell.execute_reply":"2025-03-27T12:04:38.711002Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-a111f3c78306>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Tokenize a sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print tokenized output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2859\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2860\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2968\u001b[0m             )\n\u001b[1;32m   2969\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2971\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3037\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3038\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2760\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2762\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2763\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."],"ename":"ValueError","evalue":"Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"print(dataset[\"train\"][3][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:59:54.695440Z","iopub.execute_input":"2025-03-27T11:59:54.695881Z","iopub.status.idle":"2025-03-27T11:59:54.703256Z","shell.execute_reply.started":"2025-03-27T11:59:54.695846Z","shell.execute_reply":"2025-03-27T11:59:54.701806Z"}},"outputs":[{"name":"stdout","text":" Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install transformers datasets tokenizers\n!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n!unzip -qq cornell_movie_dialogs_corpus.zip\n!rm cornell_movie_dialogs_corpus.zip\n!mkdir datasets\n!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets","metadata":{"id":"tHktDmcxoQBC","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:01.245069Z","iopub.execute_input":"2025-03-27T04:45:01.245470Z","iopub.status.idle":"2025-03-27T04:45:08.404129Z","shell.execute_reply.started":"2025-03-27T04:45:01.245441Z","shell.execute_reply":"2025-03-27T04:45:08.402689Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.21.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n--2025-03-27 04:45:06--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\nResolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\nConnecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip [following]\n--2025-03-27 04:45:06--  https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\nConnecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9916637 (9.5M) [application/zip]\nSaving to: ‘cornell_movie_dialogs_corpus.zip’\n\ncornell_movie_dialo 100%[===================>]   9.46M  37.1MB/s    in 0.3s    \n\n2025-03-27 04:45:07 (37.1 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport transformers, datasets\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"id":"zsvxdkMmPMsg","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:40:41.450798Z","iopub.execute_input":"2025-03-27T13:40:41.451107Z","iopub.status.idle":"2025-03-27T13:40:41.455609Z","shell.execute_reply.started":"2025-03-27T13:40:41.451085Z","shell.execute_reply":"2025-03-27T13:40:41.454848Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 1 ) Tokenization (Word Piece Tokenizer)\n\n[Huggingface WordPieceTokenizer](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n\nThe tokenizer's primary job is to split the input text into smaller tokens. These tokens are usually words, subwords (WordPiece tokens), or characters, depending on the specific tokenizer and its configuration.\n\nSubword Tokenization (WordPiece): BERT often uses subword tokenization, where words are further divided into smaller units called subword tokens. For instance, \"unhappiness\" might be broken down into [\"un\", \"##hap\", \"##piness\"]\n\n\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\n\n**score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)**","metadata":{"id":"DyRDEfhtoDRk"}},{"cell_type":"markdown","source":"## 1.1 Tokenizer from Scratch","metadata":{"id":"kKAlXUvhxC5Y"}},{"cell_type":"code","source":"from collections import defaultdict\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ncorpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]\n\n### get the frequency of each word ###\nword_freqs = defaultdict(int)\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    print(new_words)\n    for word in new_words:\n        word_freqs[word] += 1\n\nprint(f\"\\nFinal Word Frequency: {word_freqs}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvotJZcyxMai","executionInfo":{"status":"ok","timestamp":1702700487200,"user_tz":-480,"elapsed":615,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"0ac5ef7c-98d0-41cb-9af6-16ae03ab44e9","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:24.881796Z","iopub.execute_input":"2025-03-27T04:45:24.882501Z","iopub.status.idle":"2025-03-27T04:45:34.106453Z","shell.execute_reply.started":"2025-03-27T04:45:24.882448Z","shell.execute_reply":"2025-03-27T04:45:34.105300Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e76de1caba34ddc9e7246cffdf8f326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564be3bdc488455db77b6770d9de2e6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ca6911e7b943f38d8cb4c542744f96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f7fd87a5744953b2680e1c39476761"}},"metadata":{}},{"name":"stdout","text":"['This', 'is', 'the', 'Hugging', 'Face', 'Course', '.']\n['This', 'chapter', 'is', 'about', 'tokenization', '.']\n['This', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.']\n['Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n\nFinal Word Frequency: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"### split all word into alphabet ###\nalphabet = []\nfor word in word_freqs.keys():\n    if word[0] not in alphabet:\n        alphabet.append(word[0])\n    for letter in word[1:]:\n        if f\"##{letter}\" not in alphabet:\n            alphabet.append(f\"##{letter}\")\n\nalphabet.sort()\nprint(f'All alphabets: {alphabet}')\n\n### insert special token and subword ###\nvocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\nsplits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\nprint(f'\\nSplitted Words: {splits}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIHFIJwpxcbD","executionInfo":{"status":"ok","timestamp":1702700504613,"user_tz":-480,"elapsed":522,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"d0f83212-f6b0-4405-dc3f-c4b4cc64f039","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:39.838895Z","iopub.execute_input":"2025-03-27T04:45:39.839558Z","iopub.status.idle":"2025-03-27T04:45:39.851270Z","shell.execute_reply.started":"2025-03-27T04:45:39.839525Z","shell.execute_reply":"2025-03-27T04:45:39.850242Z"}},"outputs":[{"name":"stdout","text":"All alphabets: ['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n\nSplitted Words: {'This': ['T', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 'Face': ['F', '##a', '##c', '##e'], 'Course': ['C', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['a', '##b', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['a', '##b', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":" ### compute score for merging ###\ndef compute_pair_scores(splits):\n    letter_freqs = defaultdict(int)\n    pair_freqs = defaultdict(int)\n\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            letter_freqs[split[0]] += freq\n            continue\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i + 1])\n            letter_freqs[split[i]] += freq\n            pair_freqs[pair] += freq\n        letter_freqs[split[-1]] += freq\n\n    scores = {\n        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n        for pair, freq in pair_freqs.items()\n    }\n    return scores\n\npair_scores = compute_pair_scores(splits)\nprint(f'Scores for each Pair: {pair_scores}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5svp-f028FO","executionInfo":{"status":"ok","timestamp":1702700508853,"user_tz":-480,"elapsed":346,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9af94a71-8447-411e-8449-675050eb69ac","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:42.348766Z","iopub.execute_input":"2025-03-27T04:45:42.349169Z","iopub.status.idle":"2025-03-27T04:45:42.359791Z","shell.execute_reply.started":"2025-03-27T04:45:42.349136Z","shell.execute_reply":"2025-03-27T04:45:42.358739Z"}},"outputs":[{"name":"stdout","text":"Scores for each Pair: {('T', '##h'): 0.125, ('##h', '##i'): 0.03409090909090909, ('##i', '##s'): 0.02727272727272727, ('i', '##s'): 0.1, ('t', '##h'): 0.03571428571428571, ('##h', '##e'): 0.011904761904761904, ('H', '##u'): 0.1, ('##u', '##g'): 0.05, ('##g', '##g'): 0.0625, ('##g', '##i'): 0.022727272727272728, ('##i', '##n'): 0.01652892561983471, ('##n', '##g'): 0.022727272727272728, ('F', '##a'): 0.14285714285714285, ('##a', '##c'): 0.07142857142857142, ('##c', '##e'): 0.023809523809523808, ('C', '##o'): 0.07692307692307693, ('##o', '##u'): 0.046153846153846156, ('##u', '##r'): 0.022222222222222223, ('##r', '##s'): 0.022222222222222223, ('##s', '##e'): 0.004761904761904762, ('c', '##h'): 0.125, ('##h', '##a'): 0.017857142857142856, ('##a', '##p'): 0.07142857142857142, ('##p', '##t'): 0.07142857142857142, ('##t', '##e'): 0.013605442176870748, ('##e', '##r'): 0.026455026455026454, ('a', '##b'): 0.2, ('##b', '##o'): 0.038461538461538464, ('##u', '##t'): 0.02857142857142857, ('t', '##o'): 0.04395604395604396, ('##o', '##k'): 0.07692307692307693, ('##k', '##e'): 0.047619047619047616, ('##e', '##n'): 0.017316017316017316, ('##n', '##i'): 0.01652892561983471, ('##i', '##z'): 0.09090909090909091, ('##z', '##a'): 0.07142857142857142, ('##a', '##t'): 0.04081632653061224, ('##t', '##i'): 0.025974025974025976, ('##i', '##o'): 0.013986013986013986, ('##o', '##n'): 0.013986013986013986, ('s', '##e'): 0.031746031746031744, ('##e', '##c'): 0.023809523809523808, ('##c', '##t'): 0.07142857142857142, ('s', '##h'): 0.041666666666666664, ('##h', '##o'): 0.009615384615384616, ('##o', '##w'): 0.07692307692307693, ('##w', '##s'): 0.05, ('##e', '##v'): 0.047619047619047616, ('##v', '##e'): 0.047619047619047616, ('##r', '##a'): 0.047619047619047616, ('##a', '##l'): 0.02040816326530612, ('##z', '##e'): 0.023809523809523808, ('a', '##l'): 0.02857142857142857, ('##l', '##g'): 0.03571428571428571, ('##g', '##o'): 0.019230769230769232, ('##o', '##r'): 0.008547008547008548, ('##r', '##i'): 0.010101010101010102, ('##i', '##t'): 0.012987012987012988, ('##t', '##h'): 0.017857142857142856, ('##h', '##m'): 0.125, ('##m', '##s'): 0.1, ('H', '##o'): 0.038461538461538464, ('##o', '##p'): 0.038461538461538464, ('##p', '##e'): 0.023809523809523808, ('##e', '##f'): 0.047619047619047616, ('##f', '##u'): 0.2, ('##u', '##l'): 0.02857142857142857, ('##l', '##l'): 0.04081632653061224, ('##l', '##y'): 0.07142857142857142, ('y', '##o'): 0.07692307692307693, ('w', '##i'): 0.09090909090909091, ('##i', '##l'): 0.012987012987012988, ('b', '##e'): 0.047619047619047616, ('##b', '##l'): 0.07142857142857142, ('##l', '##e'): 0.006802721088435374, ('u', '##n'): 0.09090909090909091, ('##n', '##d'): 0.06818181818181818, ('##d', '##e'): 0.011904761904761904, ('##s', '##t'): 0.014285714285714285, ('##t', '##a'): 0.02040816326530612, ('##a', '##n'): 0.012987012987012988, ('h', '##o'): 0.07692307692307693, ('##e', '##y'): 0.023809523809523808, ('a', '##r'): 0.022222222222222223, ('##r', '##e'): 0.005291005291005291, ('t', '##r'): 0.015873015873015872, ('##a', '##i'): 0.012987012987012988, ('##n', '##e'): 0.008658008658008658, ('##e', '##d'): 0.011904761904761904, ('a', '##n'): 0.01818181818181818, ('g', '##e'): 0.047619047619047616, ('##n', '##s'): 0.00909090909090909}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### finding pair with best score ###\nbest_pair = \"\"\nmax_score = None\nfor pair, score in pair_scores.items():\n    if max_score is None or max_score < score:\n        best_pair = pair\n        max_score = score\n\nprint(best_pair, max_score)\nvocab.append(\"ab\")\n\n### merge pair ###\ndef merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        i = 0\n        while i < len(split) - 1:\n            if split[i] == a and split[i + 1] == b:\n                merge = a + b[2:] if b.startswith(\"##\") else a + b\n                split = split[:i] + [merge] + split[i + 2 :]\n            else:\n                i += 1\n        splits[word] = split\n    return splits\n\nsplits = merge_pair(\"a\", \"##b\", splits)\nprint(splits[\"about\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkcQKo1r2-c5","executionInfo":{"status":"ok","timestamp":1702700639209,"user_tz":-480,"elapsed":327,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"40267815-b5c2-4148-cdbc-8a8cb211cf65","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:45.598910Z","iopub.execute_input":"2025-03-27T04:45:45.599317Z","iopub.status.idle":"2025-03-27T04:45:45.612367Z","shell.execute_reply.started":"2025-03-27T04:45:45.599286Z","shell.execute_reply":"2025-03-27T04:45:45.611106Z"}},"outputs":[{"name":"stdout","text":"('a', '##b') 0.2\n['ab', '##o', '##u', '##t']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"### keep looping to merge more pair\nvocab_size = 70\nwhile len(vocab) < vocab_size:\n    scores = compute_pair_scores(splits)\n    best_pair, max_score = \"\", None\n    for pair, score in scores.items():\n        if max_score is None or max_score < score:\n            best_pair = pair\n            max_score = score\n    splits = merge_pair(*best_pair, splits)\n    new_token = (\n        best_pair[0] + best_pair[1][2:]\n        if best_pair[1].startswith(\"##\")\n        else best_pair[0] + best_pair[1]\n    )\n    vocab.append(new_token)\n\nprint(f'Final Vocab: {vocab}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bc6S6Tj93sh8","executionInfo":{"status":"ok","timestamp":1702700824732,"user_tz":-480,"elapsed":350,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"1afabfb5-50ce-4752-f07f-2776d62a5c4e","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:48.494456Z","iopub.execute_input":"2025-03-27T04:45:48.494819Z","iopub.status.idle":"2025-03-27T04:45:48.509143Z","shell.execute_reply.started":"2025-03-27T04:45:48.494791Z","shell.execute_reply":"2025-03-27T04:45:48.507943Z"}},"outputs":[{"name":"stdout","text":"Final Vocab: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"### ro encode a word ###\ndef encode_word(word):\n    tokens = []\n    while len(word) > 0:\n        i = len(word)\n        while i > 0 and word[:i] not in vocab:\n            i -= 1\n        if i == 0:\n            return [\"[UNK]\"]\n        tokens.append(word[:i])\n        word = word[i:]\n        if len(word) > 0:\n            word = f\"##{word}\"\n    return tokens\n\nprint(encode_word(\"Hugging\"))\nprint(encode_word(\"HOgging\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFVyHHe84Vbi","executionInfo":{"status":"ok","timestamp":1702700889155,"user_tz":-480,"elapsed":357,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"afb79be0-eda1-40a3-8a77-dc2f2078019b","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:51.114640Z","iopub.execute_input":"2025-03-27T04:45:51.114975Z","iopub.status.idle":"2025-03-27T04:45:51.125880Z","shell.execute_reply.started":"2025-03-27T04:45:51.114950Z","shell.execute_reply":"2025-03-27T04:45:51.124380Z"}},"outputs":[{"name":"stdout","text":"['Hugg', '##i', '##n', '##g']\n['[UNK]']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 1.2 Tokenizer Training","metadata":{"id":"5jaQsRw4xC-x"}},{"cell_type":"code","source":"### data processing\nMAX_LEN = 64\n\n### loading all data into memory\ncorpus_movie_conv = './datasets/movie_conversations.txt'\ncorpus_movie_lines = './datasets/movie_lines.txt'\nwith open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n    conv = c.readlines()\nwith open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n    lines = l.readlines()\n\n### splitting text using special lines\nlines_dic = {}\nfor line in lines:\n    objects = line.split(\" +++$+++ \")\n    lines_dic[objects[0]] = objects[-1]\n\n### generate question answer pairs\npairs = []\nfor con in conv:\n    ids = eval(con.split(\" +++$+++ \")[-1])\n    for i in range(len(ids)):\n        qa_pairs = []\n\n        if i == len(ids) - 1:\n            break\n\n        first = lines_dic[ids[i]].strip()\n        second = lines_dic[ids[i+1]].strip()\n\n        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n        pairs.append(qa_pairs)\n\n# sample\nprint(pairs[20])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ER4dwYJDoZFU","executionInfo":{"status":"ok","timestamp":1665133698134,"user_tz":-480,"elapsed":1961,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c0ca8f0c-4610-4dc2-f386-09be4d2f139f","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:45:53.244158Z","iopub.execute_input":"2025-03-27T04:45:53.244560Z","iopub.status.idle":"2025-03-27T04:45:56.430571Z","shell.execute_reply.started":"2025-03-27T04:45:53.244528Z","shell.execute_reply":"2025-03-27T04:45:56.429384Z"}},"outputs":[{"name":"stdout","text":"[\"I really, really, really wanna go, but I can't. Not unless my sister goes.\", \"I'm workin' on it. But she doesn't seem to be goin' for him.\"]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# WordPiece tokenizer\n\n### save data as txt file\nos.mkdir('./data')\ntext_data = []\nfile_count = 0\n\nfor sample in tqdm.tqdm([x[0] for x in pairs]):\n    text_data.append(sample)\n\n    # once we hit the 10K mark, save to file\n    if len(text_data) == 10000:\n        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(text_data))\n        text_data = []\n        file_count += 1\n\npaths = [str(x) for x in Path('./data').glob('**/*.txt')]\nprint(len(paths))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4K2nIFalnOz","executionInfo":{"status":"ok","timestamp":1665133759805,"user_tz":-480,"elapsed":868,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9cda79b0-7da1-467a-ba83-09f1f0f9095d","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:46:00.054506Z","iopub.execute_input":"2025-03-27T04:46:00.055166Z","iopub.status.idle":"2025-03-27T04:46:00.280463Z","shell.execute_reply.started":"2025-03-27T04:46:00.055129Z","shell.execute_reply":"2025-03-27T04:46:00.278771Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 221616/221616 [00:00<00:00, 1237526.71it/s]","output_type":"stream"},{"name":"stdout","text":"22\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"### training own tokenizer\ntokenizer = BertWordPieceTokenizer(\n    clean_text=True,\n    handle_chinese_chars=False,\n    strip_accents=False,\n    lowercase=True\n)\n\ntokenizer.train(\n    files=paths,\n    vocab_size=30_000,\n    min_frequency=5,\n    limit_alphabet=1000,\n    wordpieces_prefix='##',\n    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n    )\n\nos.mkdir('./bert-it-1')\ntokenizer.save_model('./bert-it-1', 'bert-it')\ntokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\ntoken_ids = tokenizer('I like surfboarding!')['input_ids']\nprint(token_ids)\nprint(tokenizer.convert_ids_to_tokens(token_ids))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzMa6PMWrJZj","executionInfo":{"status":"ok","timestamp":1665133790644,"user_tz":-480,"elapsed":3975,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9ba1f2ff-c7c5-42a3-a4d3-5de0ea35b338","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:46:02.534493Z","iopub.execute_input":"2025-03-27T04:46:02.534856Z","iopub.status.idle":"2025-03-27T04:46:06.923321Z","shell.execute_reply.started":"2025-03-27T04:46:02.534828Z","shell.execute_reply":"2025-03-27T04:46:06.922071Z"}},"outputs":[{"name":"stdout","text":"[1, 48, 250, 4033, 3588, 154, 5, 2]\n['[CLS]', 'i', 'like', 'surf', '##board', '##ing', '!', '[SEP]']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1925: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# 2) Pre-processing","metadata":{"id":"sbBU9BTdvlQ0"}},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, data_pair, tokenizer, seq_len=64):\n\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.corpus_lines = len(data_pair)\n        self.lines = data_pair\n\n    def __len__(self):\n        return self.corpus_lines\n\n    def __getitem__(self, item):\n\n        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n        t1, t2, is_next_label = self.get_sent(item)\n\n        # Step 2: replace random words in sentence with mask / random words\n        t1_random, t1_label = self.random_word(t1)\n        t2_random, t2_label = self.random_word(t2)\n\n        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n        # Adding PAD token for labels\n        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n\n        # Step 4: combine sentence 1 and 2 as one input\n        # adding PAD tokens to make the sentence same length as seq_len\n        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n        bert_input = (t1 + t2)[:self.seq_len]\n        bert_label = (t1_label + t2_label)[:self.seq_len]\n        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n\n        output = {\"bert_input\": bert_input,\n                  \"bert_label\": bert_label,\n                  \"segment_label\": segment_label,\n                  \"is_next\": is_next_label}\n\n        return {key: torch.tensor(value) for key, value in output.items()}\n\n    def random_word(self, sentence):\n        tokens = sentence.split()\n        output_label = []\n        output = []\n\n        # 15% of the tokens would be replaced\n        for i, token in enumerate(tokens):\n            prob = random.random()\n\n            # remove cls and sep token\n            token_id = self.tokenizer(token)['input_ids'][1:-1]\n\n            # 15% chance of altering token\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% chance change token to mask token\n                if prob < 0.8:\n                    for i in range(len(token_id)):\n                        output.append(self.tokenizer.vocab['[MASK]'])\n\n                # 10% chance change token to random token\n                elif prob < 0.9:\n                    for i in range(len(token_id)):\n                        output.append(random.randrange(len(self.tokenizer.vocab)))\n\n                # 10% chance change token to current token\n                else:\n                    output.append(token_id)\n\n                output_label.append(token_id)\n\n            else:\n                output.append(token_id)\n                for i in range(len(token_id)):\n                    output_label.append(0)\n\n        # flattening\n        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n        assert len(output) == len(output_label)\n        return output, output_label\n\n    def get_sent(self, index):\n        '''return random sentence pair'''\n        t1, t2 = self.get_corpus_line(index)\n\n        # negative or positive pair, for next sentence prediction\n        if random.random() > 0.5:\n            return t1, t2, 1\n        else:\n            return t1, self.get_random_line(), 0\n\n    def get_corpus_line(self, item):\n        '''return sentence pair'''\n        return self.lines[item][0], self.lines[item][1]\n\n    def get_random_line(self):\n        '''return random single sentence'''\n        return self.lines[random.randrange(len(self.lines))][1]","metadata":{"id":"bXCsfR3tmajw","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:46:06.924775Z","iopub.execute_input":"2025-03-27T04:46:06.925153Z","iopub.status.idle":"2025-03-27T04:46:06.945575Z","shell.execute_reply.started":"2025-03-27T04:46:06.925117Z","shell.execute_reply":"2025-03-27T04:46:06.943978Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# test\nprint(\"\\n\")\ntrain_data = BERTDataset(pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\nsample_data = next(iter(train_loader))\nprint('Batch Size', sample_data['bert_input'].size())\n\n# 3 is MASK\nresult = train_data[random.randrange(len(train_data))]\nresult","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RD5ta6KA_wK","executionInfo":{"status":"ok","timestamp":1665133798321,"user_tz":-480,"elapsed":4637,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"3b408feb-a0ff-4d66-89a7-6e7e32a57ee3","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:46:14.318742Z","iopub.execute_input":"2025-03-27T04:46:14.319141Z","iopub.status.idle":"2025-03-27T04:46:14.531092Z","shell.execute_reply.started":"2025-03-27T04:46:14.319110Z","shell.execute_reply":"2025-03-27T04:46:14.529987Z"}},"outputs":[{"name":"stdout","text":"\n\nBatch Size torch.Size([32, 64])\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'bert_input': tensor([   1,  302,  213,   34,  146,   11,  287,  742,  249,  253,  661,  711,\n          192,  208, 3743,   17,    2,    3,    3,    3,    3,  146,   16,   16,\n          237,   48,   11,   52,  213,  457,  173,  215,   40, 5490,   17,    2,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]),\n 'bert_label': tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,  48,  11,  43, 535,   0,   0,   0, 237,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0]),\n 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'is_next': tensor(1)}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport transformers, datasets\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport itertools\nimport torch\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer\nimport nltk\n\n# Download sentence tokenizer\nnltk.download(\"punkt\")\n\n\nclass BERTWikiTextDataset(Dataset):\n    def __init__(self, tokenizer, seq_len=128):\n        \"\"\"\n        Prepares WikiText-2 (small) dataset for BERT pretraining.\n        Includes Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n\n        # Load small WikiText dataset\n        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")[\"train\"]\n\n        # Extract raw text and split into sentences\n        self.sentences = []\n        for entry in dataset[\"text\"]:\n            self.sentences.extend(nltk.sent_tokenize(entry.strip()))\n\n        # Remove empty sentences\n        self.sentences = [s for s in self.sentences if s.strip()]\n        self.corpus_lines = len(self.sentences) - 1  # Last sentence can't form a pair\n\n    def __len__(self):\n        return self.corpus_lines\n\n    def __getitem__(self, index):\n        \"\"\"Returns a dictionary with tokenized inputs for BERT pretraining.\"\"\"\n        # Get sentence pair (t1, t2) for Next Sentence Prediction (NSP)\n        t1, t2, is_next_label = self.get_sent(index)\n\n        # Tokenize with BERT tokenizer (convert to token IDs)\n        t1_tokens = self.tokenizer.tokenize(t1)\n        t2_tokens = self.tokenizer.tokenize(t2)\n\n        # Apply BERT-style word masking (MLM)\n        t1_random, t1_label = self.random_word(t1_tokens)\n        t2_random, t2_label = self.random_word(t2_tokens)\n\n        # Add CLS and SEP tokens\n        t1 = [\"[CLS]\"] + t1_random + [\"[SEP]\"]\n        t2 = t2_random + [\"[SEP]\"]\n        t1_label = [\"[PAD]\"] + t1_label + [\"[PAD]\"]\n        t2_label = t2_label + [\"[PAD]\"]\n\n        # Convert tokens to IDs\n        t1_ids = self.tokenizer.convert_tokens_to_ids(t1)\n        t2_ids = self.tokenizer.convert_tokens_to_ids(t2)\n        t1_label_ids = self.tokenizer.convert_tokens_to_ids(t1_label)\n        t2_label_ids = self.tokenizer.convert_tokens_to_ids(t2_label)\n\n        # Combine into single sequence\n        segment_label = ([1] * len(t1_ids) + [2] * len(t2_ids))[:self.seq_len]\n        bert_input = (t1_ids + t2_ids)[:self.seq_len]\n        bert_label = (t1_label_ids + t2_label_ids)[:self.seq_len]\n\n        # Padding to seq_len\n        pad_id = self.tokenizer.pad_token_id\n        padding = [pad_id] * (self.seq_len - len(bert_input))\n\n        bert_input.extend(padding)\n        bert_label.extend(padding)\n        segment_label.extend(padding)\n\n        output = {\n            \"bert_input\": torch.tensor(bert_input),\n            \"bert_label\": torch.tensor(bert_label),\n            \"segment_label\": torch.tensor(segment_label),\n            \"is_next\": torch.tensor(is_next_label),\n        }\n\n        return output\n\n    def random_word(self, tokens):\n        \"\"\"Applies BERT-style word masking for Masked Language Modeling (MLM).\"\"\"\n        output_label = []\n        output = []\n\n        for token in tokens:\n            prob = random.random()\n\n            # 15% chance of being masked\n            if prob < 0.15:\n                prob /= 0.15\n\n                if prob < 0.8:\n                    output.append(\"[MASK]\")  # 80% mask token\n                elif prob < 0.9:\n                    output.append(random.choice(list(self.tokenizer.vocab.keys())))  # 10% random word\n                else:\n                    output.append(token)  # 10% keep original\n\n                output_label.append(token)\n            else:\n                output.append(token)\n                output_label.append(\"[PAD]\")  # No prediction needed\n\n        return output, output_label\n\n    def get_sent(self, index):\n        \"\"\"Returns a consecutive sentence pair or a randomly chosen negative pair.\"\"\"\n        t1 = self.sentences[index]\n\n        if random.random() > 0.5 and index + 1 < len(self.sentences):\n            return t1, self.sentences[index + 1], 1  # Positive pair\n        else:\n            return t1, self.get_random_sentence(), 0  # Negative pair\n\n    def get_random_sentence(self):\n        \"\"\"Returns a random sentence from WikiText dataset.\"\"\"\n        return random.choice(self.sentences)\n\n\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ndataset = BERTWikiTextDataset(tokenizer=tokenizer, seq_len=128)\nprint(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:40:59.734147Z","iopub.execute_input":"2025-03-27T13:40:59.734455Z","iopub.status.idle":"2025-03-27T13:41:04.958353Z","shell.execute_reply.started":"2025-03-27T13:40:59.734433Z","shell.execute_reply":"2025-03-27T13:41:04.957328Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n{'bert_input': tensor([  101,  1027, 11748,  4801,   103, 11906,  3523,  1027,   102, 12411,\n         5558,  2053, 11748,  4801,  4360,  1017,   103, 24004,  2890, 27108,\n         5732, 11906,   103,   103,  1024,   103,  1806,  1671, 30222, 30218,\n        30259, 30227, 30255, 30258, 30219,  2509,  1010,  5507,  1012,   102,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0]), 'bert_label': tensor([    0,     0,     0,     0,  4360, 11906,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,  1024,  4895,     0,     0,\n            0,     0,  1006,  2887,     0,  1856,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0]), 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'is_next': tensor(1)}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\n# Set sequence length to 64\nSEQ_LEN = 128\n\n# Initialize tokenizer and dataset\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ndataset = BERTWikiTextDataset(tokenizer=tokenizer, seq_len=SEQ_LEN)\n\n# Create DataLoader\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n\n# Get one batch\nsample_data = next(iter(train_loader))\n\nprint(\"Batch Size:\", sample_data[\"bert_input\"].size())  # Should print (32, 64)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:41:24.597876Z","iopub.execute_input":"2025-03-27T13:41:24.598166Z","iopub.status.idle":"2025-03-27T13:41:30.666260Z","shell.execute_reply.started":"2025-03-27T13:41:24.598145Z","shell.execute_reply":"2025-03-27T13:41:30.665481Z"}},"outputs":[{"name":"stdout","text":"Batch Size: torch.Size([32, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# 3) Modeling","metadata":{"id":"AT-uR5BXChcM"}},{"cell_type":"code","source":"### embedding\nclass PositionalEmbedding(torch.nn.Module):\n\n    def __init__(self, d_model, max_len=128, device):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float().to(device)\n        pe.require_grad = False\n\n        for pos in range(max_len):\n            # for each dimension of the each position\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n\n        # include the batch size\n        self.pe = pe.unsqueeze(0)\n        # self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe\n\nclass BERTEmbedding(torch.nn.Module):\n    \"\"\"\n    BERT Embedding which is consisted with under features\n        1. TokenEmbedding : normal embedding matrix\n        2. PositionalEmbedding : adding positional information using sin, cos\n        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n        sum of all these features are output of BERTEmbedding\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_size, seq_len=128, dropout=0.1,device):\n        \"\"\"\n        :param vocab_size: total vocab size\n        :param embed_size: embedding size of token embedding\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.embed_size = embed_size\n        # (m, seq_len) --> (m, seq_len, embed_size)\n        # padding_idx is not updated during training, remains as fixed pad (0)\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0).to(device)\n        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0).to(device)\n        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def forward(self, sequence, segment_label):\n        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n        return self.dropout(x)\n\n### testing\nembed_layer = BERTEmbedding(vocab_size=len(tokenizer.vocab), embed_size=768, seq_len=128)\nembed_result = embed_layer(sample_data['bert_input'], sample_data['segment_label'])\nprint(embed_result.size())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6Nr8gMt49dF","executionInfo":{"status":"ok","timestamp":1665133801248,"user_tz":-480,"elapsed":804,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e834d002-629f-4d93-a5dd-4e56bf07ec1f","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:41:34.469617Z","iopub.execute_input":"2025-03-27T13:41:34.469943Z","iopub.status.idle":"2025-03-27T13:41:35.399874Z","shell.execute_reply.started":"2025-03-27T13:41:34.469921Z","shell.execute_reply":"2025-03-27T13:41:35.398982Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 128, 768])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### attention layers\nclass MultiHeadedAttention(torch.nn.Module):\n\n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n\n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n\n        self.query = torch.nn.Linear(d_model, d_model)\n        self.key = torch.nn.Linear(d_model, d_model)\n        self.value = torch.nn.Linear(d_model, d_model)\n        self.output_linear = torch.nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask):\n        \"\"\"\n        query, key, value of shape: (batch_size, max_len, d_model)\n        mask of shape: (batch_size, 1, 1, max_words)\n        \"\"\"\n        # (batch_size, max_len, d_model)\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n\n        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n\n        # fill 0 mask with super small number so it wont affect the softmax weight\n        # (batch_size, h, max_len, max_len)\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n        # (batch_size, h, max_len, max_len)\n        # softmax to put attention weight for all non-pad tokens\n        # max_len X max_len matrix of attention\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n\n        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n        context = torch.matmul(weights, value)\n\n        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n\n        # (batch_size, max_len, d_model)\n        return self.output_linear(context)\n\nclass FeedForward(torch.nn.Module):\n    \"Implements FFN equation\"\n\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n\n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(\n        self,\n        d_model=768,\n        heads=12,\n        feed_forward_hidden=768 * 4,\n        dropout=0.1\n        ):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = torch.nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadedAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n        # embeddings: (batch_size, max_len, d_model)\n        # encoder mask: (batch_size, 1, 1, max_len)\n        # result: (batch_size, max_len, d_model)\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        # residual layer\n        interacted = self.layernorm(interacted + embeddings)\n        # bottleneck\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n\n### testing\nmask = (sample_data['bert_input'] > 0).unsqueeze(1).repeat(1, sample_data['bert_input'].size(1), 1).unsqueeze(1)\ntransformer_block = EncoderLayer()\ntransformer_result = transformer_block(embed_result, mask)\ntransformer_result.size()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7n6FOkOBWK2","executionInfo":{"status":"ok","timestamp":1665133802282,"user_tz":-480,"elapsed":1036,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"61857fc3-c499-4a4f-a97d-2d5af268e2b1","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:41:36.824202Z","iopub.execute_input":"2025-03-27T13:41:36.824526Z","iopub.status.idle":"2025-03-27T13:41:37.463536Z","shell.execute_reply.started":"2025-03-27T13:41:36.824483Z","shell.execute_reply":"2025-03-27T13:41:37.462748Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 128, 768])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"class BERT(torch.nn.Module):\n    \"\"\"\n    BERT model : Bidirectional Encoder Representations from Transformers.\n    \"\"\"\n\n    def __init__(self, vocab_size, d_model=512, n_layers=6, heads=8, dropout=0.1):\n        \"\"\"\n        :param vocab_size: vocab_size of total words\n        :param hidden: BERT model hidden size\n        :param n_layers: numbers of Transformer blocks(layers)\n        :param attn_heads: number of attention heads\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.heads = heads\n\n        # paper noted they used 4*hidden_size for ff_network_hidden_size\n        self.feed_forward_hidden = d_model * 4\n\n        # embedding for BERT, sum of positional, segment, token embeddings\n        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n\n        # multi-layers transformer blocks, deep network\n        self.encoder_blocks = torch.nn.ModuleList(\n            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, segment_info):\n        # attention masking for padded token\n        # (batch_size, 1, seq_len, seq_len)\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        # embedding the indexed sequence to sequence of vectors\n        x = self.embedding(x, segment_info)\n\n        # running over multiple transformer blocks\n        for encoder in self.encoder_blocks:\n            x = encoder.forward(x, mask)\n        return x\n\nclass NextSentencePrediction(torch.nn.Module):\n    \"\"\"\n    2-class classification model : is_next, is_not_next\n    \"\"\"\n\n    def __init__(self, hidden):\n        \"\"\"\n        :param hidden: BERT model output size\n        \"\"\"\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, 2)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        # use only the first token which is the [CLS]\n        return self.softmax(self.linear(x[:, 0]))\n\nclass MaskedLanguageModel(torch.nn.Module):\n    \"\"\"\n    predicting origin token from masked input sequence\n    n-class classification problem, n-class = vocab_size\n    \"\"\"\n\n    def __init__(self, hidden, vocab_size):\n        \"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, vocab_size)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x))\n\nclass BERTLM(torch.nn.Module):\n    \"\"\"\n    BERT Language Model\n    Next Sentence Prediction Model + Masked Language Model\n    \"\"\"\n\n    def __init__(self, bert: BERT, vocab_size):\n        \"\"\"\n        :param bert: BERT model which should be trained\n        :param vocab_size: total vocab size for masked_lm\n        \"\"\"\n\n        super().__init__()\n        self.bert = bert\n        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n\n    def forward(self, x, segment_label):\n        x = self.bert(x, segment_label)\n        return self.next_sentence(x), self.mask_lm(x)\n\n### test\nbert_model = BERT(len(tokenizer.vocab))\nbert_result = bert_model(sample_data['bert_input'], sample_data['segment_label'])\nprint(bert_result.size())\n\nbert_lm = BERTLM(bert_model, len(tokenizer.vocab))\nfinal_result = bert_lm(sample_data['bert_input'], sample_data['segment_label'])\nprint(final_result[0].size(), final_result[1].size())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4am76N6Cimj","executionInfo":{"status":"ok","timestamp":1665133817819,"user_tz":-480,"elapsed":15539,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e0b849c5-50c2-4f15-b1f0-f08fbe95fc90","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:41:40.105986Z","iopub.execute_input":"2025-03-27T13:41:40.106279Z","iopub.status.idle":"2025-03-27T13:41:45.419242Z","shell.execute_reply.started":"2025-03-27T13:41:40.106258Z","shell.execute_reply":"2025-03-27T13:41:45.418529Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 128, 512])\ntorch.Size([32, 2]) torch.Size([32, 128, 30522])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 4) Training","metadata":{"id":"nnp98JEZWwgN"}},{"cell_type":"code","source":"### optimizer\nclass ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.n_warmup_steps = n_warmup_steps\n        self.n_current_steps = 0\n        self.init_lr = np.power(d_model, -0.5)\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n    def zero_grad(self):\n        \"Zero out the gradients by the inner optimizer\"\n        self._optimizer.zero_grad()\n\n    def _get_lr_scale(self):\n        return np.min([\n            np.power(self.n_current_steps, -0.5),\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_current_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr","metadata":{"id":"0PPi4L1sCjBf","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:41:45.420094Z","iopub.execute_input":"2025-03-27T13:41:45.420394Z","iopub.status.idle":"2025-03-27T13:41:45.426050Z","shell.execute_reply.started":"2025-03-27T13:41:45.420358Z","shell.execute_reply":"2025-03-27T13:41:45.425291Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"### trainer\nclass BERTTrainer:\n    def __init__(\n        self,\n        model,\n        train_dataloader,\n        test_dataloader=None,\n        lr= 1e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        warmup_steps=10000,\n        log_freq=10,\n        device='cuda'\n        ):\n\n        self.device = device\n        self.model = model\n        self.train_data = train_dataloader\n        self.test_data = test_dataloader\n\n        # Setting the Adam optimizer with hyper-param\n        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n        self.optim_schedule = ScheduledOptim(\n            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n            )\n\n        # Using Negative Log Likelihood Loss function for predicting the masked_token\n        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n        self.log_freq = log_freq\n        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n\n    def train(self, epoch):\n        self.iteration(epoch, self.train_data)\n\n    def test(self, epoch):\n        self.iteration(epoch, self.test_data, train=False)\n\n    def iteration(self, epoch, data_loader, train=True):\n\n        avg_loss = 0.0\n        total_correct = 0\n        total_element = 0\n\n        mode = \"train\" if train else \"test\"\n\n        # progress bar\n        data_iter = tqdm.tqdm(\n            enumerate(data_loader),\n            desc=\"EP_%s:%d\" % (mode, epoch),\n            total=len(data_loader),\n            bar_format=\"{l_bar}{r_bar}\"\n        )\n\n        for i, data in data_iter:\n\n            # 0. batch_data will be sent into the device(GPU or cpu)\n            data = {key: value.to(self.device) for key, value in data.items()}\n\n            # 1. forward the next_sentence_prediction and masked_lm model\n            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n\n            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n\n            # 2-2. NLLLoss of predicting masked token word\n            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n\n            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n            loss = next_loss + mask_loss\n\n            # 3. backward and optimization only in train\n            if train:\n                self.optim_schedule.zero_grad()\n                loss.backward()\n                self.optim_schedule.step_and_update_lr()\n\n            # next sentence prediction accuracy\n            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n            avg_loss += loss.item()\n            total_correct += correct\n            total_element += data[\"is_next\"].nelement()\n\n            post_fix = {\n                \"epoch\": epoch,\n                \"iter\": i,\n                \"avg_loss\": avg_loss / (i + 1),\n                \"avg_acc\": total_correct / total_element * 100,\n                \"loss\": loss.item()\n            }\n\n            if i % self.log_freq == 0:\n                data_iter.write(str(post_fix))\n        print(\n            f\"EP{epoch}, {mode}: \\\n            avg_loss={avg_loss / len(data_iter)}, \\\n            total_acc={total_correct * 100.0 / total_element}\"\n\n\ntrain_data = BERTWikiTextDataset( seq_len=128, tokenizer=tokenizer)\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\nbert_model = BERT(len(tokenizer.vocab))\nbert_lm = BERTLM(bert_model, len(tokenizer.vocab))\nbert_trainer = BERTTrainer(bert_lm, train_loader, device='cuda')\nepochs = 2\n\nfor epoch in range(epochs):\n    bert_trainer.train(epoch)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-27T14:23:31.595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tqdm\nfrom torch.optim import Adam\n\nclass BERTTrainer:\n    def __init__(\n        self,\n        model,\n        train_dataloader,\n        test_dataloader=None,\n        lr=1e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        warmup_steps=10000,\n        log_freq=10,\n        device='cuda'\n    ):\n        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)  # Move model to device\n        self.train_data = train_dataloader\n        self.test_data = test_dataloader\n\n        # Optimizer setup\n        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n        self.optim_schedule = ScheduledOptim(self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps)\n\n        # Loss function (Negative Log Likelihood Loss)\n        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n\n        self.log_freq = log_freq\n        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n\n    def train(self, epoch):\n        self.iteration(epoch, self.train_data, train=True)\n\n    def test(self, epoch):\n        if self.test_data is not None:\n            self.iteration(epoch, self.test_data, train=False)\n\n    def iteration(self, epoch, data_loader, train=True):\n        avg_loss = 0.0\n        total_correct = 0\n        total_element = 0\n\n        mode = \"train\" if train else \"test\"\n\n        # Progress bar\n        data_iter = tqdm.tqdm(\n            enumerate(data_loader),\n            desc=\"EP_%s:%d\" % (mode, epoch),\n            total=len(data_loader),\n            bar_format=\"{l_bar}{r_bar}\"\n        )\n\n        for i, data in data_iter:\n            # Ensure all tensors are moved to the correct device\n            data = {key: value.to(self.device) for key, value in data.items()}\n\n            # Forward pass: Next Sentence Prediction + Masked Language Model\n            next_sent_output, mask_lm_output = self.model.forward(\n                data[\"bert_input\"], data[\"segment_label\"]\n            )\n\n            # Compute loss\n            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n            mask_lm_loss = self.criterion(\n                mask_lm_output.view(-1, mask_lm_output.size(-1)), \n                data[\"bert_label\"].view(-1)  # Ensure labels are flattened\n            )\n\n            loss = next_loss + mask_lm_loss\n            avg_loss += loss.item()\n\n            # Compute accuracy for Next Sentence Prediction\n            correct = (next_sent_output.argmax(dim=-1) == data[\"is_next\"]).sum().item()\n            total_correct += correct\n            total_element += data[\"is_next\"].numel()\n\n            # Backpropagation\n            if train:\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n                self.optim_schedule.step()\n\n            # Logging\n            if i % self.log_freq == 0:\n                data_iter.set_description(\n                    \"EP_%s:%d, Iter:%d, Loss:%0.4f, NSP_Acc:%0.2f%%\" % (\n                        mode, epoch, i, loss.item(), (total_correct / total_element) * 100\n                    )\n                )\n\n        print(\"EP%d_%s: Average Loss: %0.4f, NSP Accuracy: %0.2f%%\" % (\n            epoch, mode, avg_loss / len(data_loader), (total_correct / total_element) * 100\n        ))\n\ntrain_data = BERTWikiTextDataset( seq_len=128, tokenizer=tokenizer)\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\nbert_model = BERT(len(tokenizer.vocab))\nbert_lm = BERTLM(bert_model, len(tokenizer.vocab))\nbert_trainer = BERTTrainer(bert_lm, train_loader, device='cuda')\nepochs = 2\n\nfor epoch in range(epochs):\n    bert_trainer.train(epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:50:15.106120Z","iopub.execute_input":"2025-03-27T13:50:15.106416Z","iopub.status.idle":"2025-03-27T13:50:20.852135Z","shell.execute_reply.started":"2025-03-27T13:50:15.106394Z","shell.execute_reply":"2025-03-27T13:50:20.850993Z"}},"outputs":[{"name":"stdout","text":"Total Parameters: 50195772\n","output_type":"stream"},{"name":"stderr","text":"EP_train:0:   0%|| 0/2724 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-95921618eeb4>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mbert_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-95921618eeb4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-95921618eeb4>\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# Forward pass: Next Sentence Prediction + Masked Language Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             next_sent_output, mask_lm_output = self.model.forward(\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bert_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"segment_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             )\n","\u001b[0;32m<ipython-input-13-96c1927875d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, segment_label)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-96c1927875d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, segment_info)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# embedding the indexed sequence to sequence of vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# running over multiple transformer blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-447e47af1689>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence, segment_label)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","output_type":"error"}],"execution_count":23},{"cell_type":"markdown","source":"# 5) Reference\n\n[BERT from Scratch](https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891) | [BERT vs Roberta vs XLM](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8) | [StructBert vs Albert vs LongForm](https://towardsdatascience.com/advancing-over-bert-bigbird-convbert-dynabert-bca78a45629c) | [BART](https://medium.com/analytics-vidhya/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564)","metadata":{"id":"hvIpgHXg6HOK"}},{"cell_type":"markdown","source":"# 6) Context Guided BERT\n\n[CG-BERT REPO](https://github.com/frankaging/Quasi-Attention-ABSA/blob/main/code/model/CGBERT.py)\n\n```python\n# number of context classes\ncontext_id_map_sentihood = [\n    'location - 1 - general',\n    'location - 1 - price',\n    'location - 1 - safety',\n    'location - 1 - transit location',\n    'location - 2 - general',\n    'location - 2 - price',\n    'location - 2 - safety',\n    'location - 2 - transit location'\n]\n```\n","metadata":{"id":"KArJKXQchHNy"}},{"cell_type":"code","source":"class BERTLayerNorm(nn.Module):\n    def __init__(self, config, variance_epsilon=1e-12):\n        super(BERTLayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n        self.variance_epsilon = variance_epsilon\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n        return self.gamma * x + self.beta","metadata":{"id":"tWeg7wbchJWk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextBERTSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(ContextBERTSelfAttention, self).__init__()\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        # learnable context integration factors\n        # enforce initialization to zero as to leave the pretrain model\n        # unperturbed in the beginning\n        self.context_for_q = nn.Linear(self.attention_head_size, self.attention_head_size)\n        self.context_for_k = nn.Linear(self.attention_head_size, self.attention_head_size)\n\n        self.lambda_q_context_layer = nn.Linear(self.attention_head_size, 1, bias=False)\n        self.lambda_q_query_layer = nn.Linear(self.attention_head_size, 1, bias=False)\n        self.lambda_k_context_layer = nn.Linear(self.attention_head_size, 1, bias=False)\n        self.lambda_k_key_layer = nn.Linear(self.attention_head_size, 1, bias=False)\n\n        # zero-centered activation function, specifically for re-arch fine tunning\n        self.lambda_sig = nn.Sigmoid()\n        self.quasi_act = nn.Sigmoid()\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask,\n                # optional parameters for saving context information\n                device=None, context_embedded=None):\n\n        # (m, seq_len, hidden_dim)\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        # (m, num_head, seq_len, head_dim)\n        mixed_query_layer = self.transpose_for_scores(mixed_query_layer)\n        mixed_key_layer = self.transpose_for_scores(mixed_key_layer)\n\n        # (m, 1, hidden_dim) --> (m, num_head, 1, head_dim)\n        context_embedded = self.transpose_for_scores(context_embedded)\n        context_embedded_q = self.context_for_q(context_embedded)\n\n        # (m, num_head, 1, head_dim) --> (m, num_head, 1, 1)\n        lambda_q_context = self.lambda_q_context_layer(context_embedded_q)\n        # (m, num_head, seq_len, head_dim) --> (m, num_head, seq_len, 1)\n        lambda_q_query = self.lambda_q_query_layer(mixed_query_layer)\n        # (m, num_head, seq_len, 1)\n        lambda_q = lambda_q_context + lambda_q_query\n        lambda_q = self.lambda_sig(lambda_q)\n\n        # Q_context = (1-lambda_Q) * Q + lambda_Q * Context_Q\n        # K_context = (1-lambda_K) * K + lambda_K * Context_K\n        # the context is shared and is the same for every head.\n\n        # (m, num_head, seq_len, head_dim)\n        contextualized_query_layer = (1 - lambda_q) * mixed_query_layer + lambda_q * context_embedded_q\n\n        # repeat same for key\n        context_embedded_k = self.context_for_k(context_embedded)\n        lambda_k_context = self.lambda_k_context_layer(context_embedded_k)\n        lambda_k_key = self.lambda_k_key_layer(mixed_key_layer)\n        lambda_k = lambda_k_context + lambda_k_key\n        lambda_k = self.lambda_sig(lambda_k)\n\n        # (m, num_head, seq_len, head_dim)\n        contextualized_key_layer = (1 - lambda_k) * mixed_key_layer + lambda_k * context_embedded_k\n\n        ######################################################################\n\n        # (m, num_head, seq_len, seq_len)\n        attention_scores = torch.matmul(\n            contextualized_query_layer, contextualized_key_layer.transpose(-1, -2)\n        )\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_scores = attention_scores + attention_mask\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        attention_probs = self.dropout(attention_probs)\n\n        # (m, num_head, seq_len, seq_len)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # (m, num_head, seq_len, head_dim)\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        # (m, seq_len, num_head, head_dim)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n\n        # (m, seq_len, hidden_dim)\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        return context_layer","metadata":{"id":"i0-A5oWPha5j"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextBERTEncoder(nn.Module):\n    def __init__(self, config):\n        super(ContextBERTEncoder, self).__init__()\n\n        deep_context_transform_layer = nn.Linear(\n            2*config.hidden_size, config.hidden_size\n        )\n\n        self.context_layer = nn.ModuleList([\n            copy.deepcopy(deep_context_transform_layer) for _ in range(config.num_hidden_layers)\n        ])\n\n        layer = ContextBERTLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, device=None, context_embeddings=None):\n\n        all_encoder_layers = []\n        layer_index = 0\n        for layer_module in self.layer:\n            deep_context_hidden = torch.cat([context_embeddings, hidden_states], dim=-1)\n            deep_context_hidden = self.context_layer[layer_index](deep_context_hidden)\n            deep_context_hidden += context_embeddings\n\n            # BERT encoding\n            hidden_states = layer_module(\n                hidden_states, attention_mask, device, deep_context_hidden\n            )\n            # (n_layer, m, seq_len, hidden_dim)\n            all_encoder_layers.append(hidden_states)\n            layer_index += 1\n\n        return all_encoder_layers","metadata":{"id":"0DpFJt7QxRZE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextBertModel(nn.Module):\n    def __init__(self, config: BertConfig):\n        super(ContextBertModel, self).__init__()\n        self.embeddings = BERTEmbeddings(config)\n        self.encoder = ContextBERTEncoder(config)\n        self.pooler = ContextBERTPooler(config)\n\n        # context embedding\n        num_target = 4\n        num_aspect = 2\n        self.context_embeddings = nn.Embedding(num_target*num_aspect, config.hidden_size)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n                device=None, context_ids=None):\n\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # [batch_size, 1, 1, from_seq_length]\n        # broadcast to [batch_size, num_heads, seq_length, seq_length]\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # multiply by negative big number to make it ignore during softmax\n        extended_attention_mask = extended_attention_mask.float()\n        extended_attention_mask = (1.0 - extended_attention_mask) * -9e9\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n\n        # context embeddings\n        # [batch_size, 1, context_embedding_dim]\n        context_embedded = self.context_embeddings(context_ids).squeeze(dim=1)\n        # [batch_size, seq_len, context_embedding_dim]\n        seq_len = embedding_output.shape[1]\n        context_embedding_output = torch.stack(seq_len*[context_embedded], dim=1)\n\n        # (n_layer, m, seq_len, hidden_dim)\n        all_encoder_layers = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            device,\n            context_embedding_output\n        )\n\n        sequence_output = all_encoder_layers[-1]\n        pooled_output = self.pooler(sequence_output, attention_mask)\n        return pooled_output","metadata":{"id":"WyT5M1jCsl0d"},"outputs":[],"execution_count":null}]}